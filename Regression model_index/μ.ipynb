{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98214754-e2af-482f-81a6-1807362f830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet import ResNet50\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40419687-4e01-4466-95ee-343a4dc4a49e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebcaeb-cfab-4ce6-b891-60c479b24d29",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343a10e-d5a2-4308-8ce3-31ca5d3960ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "labels=pd.read_excel('data_u.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9652d-c68e-4601-8da9-c125d4799b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "\n",
    "cols = ['p', 'Lmin', 'a_4', 'b_4'] \n",
    "# cols = ['Lmin', 'a_4', 'b_4'] \n",
    "\n",
    "cols2 = ['u']       \n",
    "\n",
    "\n",
    "ss_input = MinMaxScaler()\n",
    "labels[cols] = ss_input.fit_transform(labels[cols])\n",
    "joblib.dump(ss_input, 'scaler_all.pkl')  \n",
    "\n",
    "\n",
    "ss_output = MinMaxScaler()\n",
    "labels[cols2] = ss_output.fit_transform(labels[cols2])\n",
    "joblib.dump(ss_output, 'ss.pkl')       \n",
    "\n",
    "\n",
    "labels.to_excel('normalized_data.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b258eb-a64d-4973-a8d2-11f4922a232f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c966d8a-cddf-49a9-98b1-86f1361b8b07",
   "metadata": {},
   "source": [
    "## Create training and testing data â€“ same as Xception_multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32e649-1908-4e5f-b1e9-380222ba0765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "image_folder = \"./images\"\n",
    "train_folder = './train'\n",
    "test_folder = './test'\n",
    "\n",
    "# if os.path.exists(train_folder):\n",
    "#     shutil.rmtree(train_folder)\n",
    "# if os.path.exists(test_folder):\n",
    "#     shutil.rmtree(test_folder)\n",
    "# os.makedirs(train_folder, exist_ok=True)\n",
    "# os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "# image_files = [i for i in os.listdir(image_folder) if i.find('jpg')>=0]\n",
    "# random.shuffle(image_files)\n",
    "\n",
    "\n",
    "# total_images = len(image_files)\n",
    "# train_images = int(total_images * 0.7)\n",
    "# test_images = total_images - train_images\n",
    "\n",
    "\n",
    "# for i, image_file in enumerate(image_files):\n",
    "#     source_path = os.path.join(image_folder, image_file)\n",
    "\n",
    "\n",
    "#     if i < train_images:\n",
    "#         destination_folder = train_folder\n",
    "#     else:\n",
    "#         destination_folder = test_folder\n",
    "\n",
    "\n",
    "#     destination_path = os.path.join(destination_folder, image_file)\n",
    "#     shutil.copyfile(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d7180-80fe-493b-8e58-1f17112fcb76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_list=[i for i in os.listdir(train_folder) if i.find('jpg')>=0]\n",
    "test_list=[i for i in os.listdir(test_folder) if i.find('jpg')>=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59541da0-9918-4c1a-a138-41e1495ec86f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_csv=labels[labels.name.isin(train_list)]\n",
    "test_csv=labels[labels.name.isin(test_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ba723e-8c70-4cd0-82d3-b14f21007a30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_list = ['p', 'Lmin', 'a_4', 'b_4', 'u']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d68a8-d237-4045-b5ca-05555821b2d7",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f7c97-6dcd-41c2-9f9a-386f916c34ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, csv_file, directory, batch_size, target_size, label_list, shuffle=True, augment=False):\n",
    "        self.csv_file = csv_file\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.label_list = label_list\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        if self.augment:\n",
    "            self.image_data_generator = ImageDataGenerator(\n",
    "                rotation_range=20,\n",
    "                width_shift_range=0.2,\n",
    "                height_shift_range=0.2,\n",
    "                shear_range=0.2,\n",
    "                zoom_range=0.2,\n",
    "                horizontal_flip=True,\n",
    "                fill_mode='nearest')\n",
    "        else:\n",
    "            self.image_data_generator = ImageDataGenerator()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.csv_file) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.csv_file.iloc[k] for k in indexes]\n",
    "        return self.__data_generation(batch)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.csv_file))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, batch):\n",
    "        X1 = np.empty((self.batch_size, *self.target_size, 3))\n",
    "        X2 = np.empty((self.batch_size, 4)) \n",
    "        y = np.empty((self.batch_size,1))  \n",
    "\n",
    "        for i, data in enumerate(batch):\n",
    "            img_path = f\"{self.directory}/{data['name']}\"\n",
    "            image = load_img(img_path, target_size=self.target_size)\n",
    "            # image = img_to_array(image) / 255.0\n",
    "            image = img_to_array(image)\n",
    "            \n",
    "\n",
    "            if self.augment:\n",
    "                image = self.image_data_generator.random_transform(image)\n",
    "            \n",
    "            X1[i] = image\n",
    "            X2[i] = data[self.label_list[:4]]  # [p'Lmin', 'a_4', 'b_4']\n",
    "            y[i] = data[self.label_list[4]]    # ['u']\n",
    "\n",
    "\n",
    "        return [X1, X2], y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd8281f-295c-4b67-b2bf-6399afcf5a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "target_size = (256, 256)\n",
    "\n",
    "train_generator = CustomDataGenerator(train_csv, './train', batch_size, target_size, label_list, shuffle=False)\n",
    "test_generator = CustomDataGenerator(test_csv, './test', batch_size, target_size, label_list, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4849d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()  \n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')  \n",
    "for device in physical_devices:  \n",
    "     print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c116ed09-6905-47ae-bfc2-d8b2be669b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "y_train = []\n",
    "\n",
    "for i in tqdm(range(len(train_generator))):\n",
    "    batch_data, batch_labels = train_generator[i]\n",
    "    y_train.append(batch_labels)\n",
    "\n",
    "y_train = np.concatenate(y_train)\n",
    "y_test = []\n",
    "\n",
    "for i in tqdm(range(len(test_generator))):\n",
    "    batch_data, batch_labels = test_generator[i]\n",
    "    y_test.append(batch_labels)\n",
    "\n",
    "y_test = np.concatenate(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf576e-77a5-4dfb-aa83-7c415bf70e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model_history(model_history,model_name):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(12,4),dpi=120)\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['mse'])+1),model_history.history['mse'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_mse'])+1),model_history.history['val_mse'])\n",
    "    axs[0].set_title('Model mse')\n",
    "    axs[0].set_ylabel('mse')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    # axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])/10)\n",
    "    # axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    # axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    fig.savefig(model_name+'_loss.jpg',dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d5aca-600b-4cce-a135-95153d2e3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate, Multiply, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_model2(hp):\n",
    "    # â€”â€” 1) Image input (299 + official preprocessing) â€”â€”\n",
    "    from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
    "    input_image = Input(shape=(299, 299, 3), name='img')\n",
    "    x_in = tf.keras.layers.Lambda(preprocess_input, name='xcep_pre')(input_image)\n",
    "\n",
    "    # Xception backbone (without top layers; GAP is generally more stable, can switch back to Flatten for a fair comparison)\n",
    "    base_model = Xception(weights='imagenet', include_top=False,\n",
    "                          input_shape=(299, 299, 3), pooling='avg')\n",
    "\n",
    "    # Freeze only BatchNorm layers to avoid updating statistics with small batches; forward pass in inference mode\n",
    "    for l in base_model.layers:\n",
    "        if isinstance(l, tf.keras.layers.BatchNormalization):\n",
    "            l.trainable = False\n",
    "    x_img = base_model(x_in, training=False)   # BN uses inference statistics\n",
    "\n",
    "    # â€”â€” 2) Numerical input branch (4 auxiliary features) + lightweight attention + stabilization â€”â€”\n",
    "    input_features = Input(shape=(4,), name='aux4')\n",
    "\n",
    "    # Attention weights (sigmoid) for feature recalibration\n",
    "    att = Dense(4, activation='sigmoid', name='attention_weights')(input_features)\n",
    "    aux = Multiply(name='aux_weighted')([input_features, att])\n",
    "\n",
    "    # Small MLP + BN + Dropout (more stable)\n",
    "    aux = Dense(32, activation='relu')(aux)\n",
    "    aux = BatchNormalization()(aux)\n",
    "    aux = Dropout(0.1)(aux)\n",
    "    aux = Dense(16, activation='relu')(aux)\n",
    "\n",
    "    # â€”â€” 3) Fusion â€”â€”\n",
    "    x = Concatenate(name='concat_img_aux')([x_img, aux])\n",
    "\n",
    "    # â€”â€” 4) Three fully connected layers: relu â†’ tanh â†’ relu (keeping your style), with a bit of L2 regularization for stability â€”â€”\n",
    "    x = Dense(hp.Int('units_0', 96, 512, step=16),\n",
    "              activation='relu', kernel_regularizer=l2(1e-5))(x)\n",
    "    x = Dense(hp.Int('units_1', 96, 512, step=16),\n",
    "              activation='tanh', kernel_regularizer=l2(1e-5))(x)\n",
    "    x = Dense(hp.Int('units_2', 96, 512, step=16),\n",
    "              activation='relu', kernel_regularizer=l2(1e-5))(x)\n",
    "\n",
    "    output = Dense(1, name='target')(x)\n",
    "\n",
    "    model = Model(inputs=[input_image, input_features], outputs=output)\n",
    "\n",
    "    # Slightly lower the minimum learning rate; Xception fine-tuning is more stable this way\n",
    "    lr = hp.Float('lr', min_value=3e-5, max_value=1e-3, sampling='LOG')\n",
    "    try:\n",
    "        # If AdamW is available in the environment, use it with mild weight decay\n",
    "        from tensorflow.keras.optimizers import AdamW\n",
    "        opt = AdamW(learning_rate=lr, weight_decay=1e-5)\n",
    "    except Exception:\n",
    "        opt = Adam(learning_rate=lr)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Tuner configuration\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "tuner = RandomSearch(\n",
    "    build_model2,\n",
    "    objective='val_loss',\n",
    "    max_trials=25,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='xception_tuning'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "tuner.search(train_generator, epochs=10, validation_data=test_generator)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "print(f\"Learning rate: {best_hps.get('lr')}\")\n",
    "print(f\"Layer 1: units={best_hps.get('units_0')}, activation=relu\")\n",
    "print(f\"Layer 2: units={best_hps.get('units_1')}, activation=tanh\")\n",
    "print(f\"Layer 3: units={best_hps.get('units_2')}, activation=relu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6525135-9abe-443b-a860-0584187b6f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"./model.ckpt\", monitor='val_loss', verbose=1,\n",
    "                                                save_best_only=True, mode='min', save_weights_only=True)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef0382c-dcc1-4cd3-a843-706688c823ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_info = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=50,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(test_generator),\n",
    "    callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df07ebf8-9f9e-43e2-9008-716a426b2afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model_history(model_info, 'Xception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7dc6e-ceb0-47f2-a424-7a5c30f31cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save('./best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9335d55-6646-4d5c-9860-371f3f8bac59",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dabeca-967d-4380-a4e9-723088c2f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('./best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe403c42-00da-4a29-88dd-5747d1bddd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ss_output = joblib.load('ss.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab5f40-960a-4b0a-9956-3d000d6de340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_test_pred=model.predict(test_generator)\n",
    "y_train_pred=model.predict(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d5d41-b832-4011-9b1c-f76161eac49a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94d05c-08e3-4574-985a-827e61cea10a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23493449-3cb6-41a1-ba32-5fd32fab5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred=ss_output.inverse_transform(y_test_pred)\n",
    "y_test_true=ss_output.inverse_transform(y_test)\n",
    "\n",
    "y_train_pred=ss_output.inverse_transform(y_train_pred)\n",
    "y_train_true=ss_output.inverse_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120183d-3cfd-41f3-b71c-13ae20d67b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === evaluation ===\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def model_result(y_train, y_pred_train, y_test, y_pred_test):\n",
    "    print('ðŸ“Š train:')\n",
    "    print('MSE:  ', mean_squared_error(y_train, y_pred_train))\n",
    "    print('MAE:  ', mean_absolute_error(y_train, y_pred_train))\n",
    "    print('RMSE: ', np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "    print('RÂ²:   ', r2_score(y_train, y_pred_train))\n",
    "    print('\\nðŸ“Š test:')\n",
    "    print('MSE:  ', mean_squared_error(y_test, y_pred_test))\n",
    "    print('MAE:  ', mean_absolute_error(y_test, y_pred_test))\n",
    "    print('RMSE: ', np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "    print('RÂ²:   ', r2_score(y_test, y_pred_test))\n",
    "\n",
    "model_result(y_train_true, y_train_pred, y_test_true, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf50e53c-75a9-4a2b-a59b-2e72e39f6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "color_list = ['red','blue','purple','orange','yellow','green',\n",
    "              'pink','aquamarine','dodgerblue','purple',\n",
    "              'red','blue','orange','yellow','green']\n",
    "\n",
    "plt.figure(figsize=(12,10), dpi=120)\n",
    "\n",
    "for i in range(1, len(label_list)+1):\n",
    "    if y_test_pred.shape[1] < i:\n",
    "        print(f\" The {i}-th label exceeds the prediction dimension, skipped\")\n",
    "        continue\n",
    "\n",
    "    plt.subplot(3, 3, i)\n",
    "    plt.scatter(y_test_pred[:, i-1].ravel(), y_test_true[:, i-1].ravel(),\n",
    "                color=color_list[i-1], alpha=0.6)\n",
    "    \n",
    "    min_val = min(y_test_pred[:, i-1].min(), y_test_true[:, i-1].min())\n",
    "    max_val = max(y_test_pred[:, i-1].max(), y_test_true[:, i-1].max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=1)\n",
    "    \n",
    "    plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    plt.title(label_list[i-1])\n",
    "    plt.xlabel('Predict')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('index.jpg', dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7c751-33f6-4f65-8ad6-56f2c897448f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df_result = pd.DataFrame({\n",
    "    'u_true': y_test_true.ravel(),\n",
    "    'u_predicted': y_test_pred.ravel()\n",
    "})\n",
    "df_result.to_excel('u_true_vs_predicted.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0d3f1-aa8b-4a23-b758-cf73827002c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e35a0bd1-74d0-4f05-8f21-7efa5b6b5ca4",
   "metadata": {},
   "source": [
    "## Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbcf1da-7a0f-4c28-9023-5098dd83f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Path Configuration =====================\n",
    "test_folder = './val_images'                  # Path to the images for prediction\n",
    "model_path = './best_model.h5'                # Trained model (image + 4 features)\n",
    "scaler_input_path = './scaler_all.pkl'        # Normalizer for the 4 input features (fitted on the training set)\n",
    "scaler_output_path = './ss.pkl'               # Output denormalizer (fitted on the training set)\n",
    "output_excel_path = 'predicted_u_output.xlsx' # Output Excel file\n",
    "\n",
    "# ===================== Load Model and Scalers =====================\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "# Read the model's expected image input size (H, W, C)\n",
    "# Note: model.inputs[0] corresponds to the image input; if your model has only one input, this is it.\n",
    "# If the model has multiple inputs, the first one is usually the image \n",
    "# (as in your training where inputs=[input_image, input_features])\n",
    "target_h = int(model.inputs[0].shape[1])\n",
    "target_w = int(model.inputs[0].shape[2])\n",
    "\n",
    "# Check whether the model already contains the Xception preprocessing layer \n",
    "# (for example, during training you named the Lambda layer 'xcep_pre')\n",
    "has_internal_preprocess = any(\n",
    "    (getattr(layer, \"name\", \"\") == \"xcep_pre\") for layer in model.layers\n",
    ")\n",
    "\n",
    "# Load scalers\n",
    "ss_input = joblib.load(scaler_input_path)    # For [porosity, Lmin, a_4, b_4]\n",
    "ss_output = joblib.load(scaler_output_path)  # For denormalizing u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877e451-94ef-4156-8ce7-cec20efacefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Image List =====================\n",
    "image_files = [f for f in os.listdir(test_folder) if f.lower().endswith('.jpg')]\n",
    "image_files = natsorted(image_files)\n",
    "\n",
    "# ===================== Helper function for prediction: compute four handcrafted features =====================\n",
    "def compute_aux_features(gray_img, black_threshold=80):\n",
    "    \"\"\"\n",
    "    gray_img: uint8, shape [H, W], range 0~255\n",
    "    Returns: porosity, Lmin, a_4, b_4\n",
    "    \"\"\"\n",
    "    H, W = gray_img.shape\n",
    "\n",
    "    # 1) Porosity (ratio of white pixels; threshold consistent with training)\n",
    "    white_pixel_count = np.sum(gray_img > black_threshold)\n",
    "    total_pixels = gray_img.size\n",
    "    porosity = white_pixel_count / total_pixels if total_pixels > 0 else 0.0\n",
    "\n",
    "    # 2) Lmin: minimum number of black pixels (<128) per row\n",
    "    black_pixel_counts_per_row = np.sum(gray_img < 128, axis=1)\n",
    "    Lmin = int(np.min(black_pixel_counts_per_row)) if black_pixel_counts_per_row.size > 0 else 0\n",
    "\n",
    "    # 3) a_4: Proportion of black pixels in region D (circle centered at image center, radius = H/2)\n",
    "    center = (W // 2, H // 2)\n",
    "    mask_total = np.ones_like(gray_img, dtype=np.uint8) * 255\n",
    "    mask_D = mask_total.copy()\n",
    "    cv2.circle(mask_D, center, H // 2, 0, -1)  # Radius = H/2\n",
    "    total_black_pixels = np.sum(gray_img < black_threshold)\n",
    "    D_black_pixels = np.sum((gray_img < black_threshold) & (mask_D > 0))\n",
    "    a_4 = (D_black_pixels / total_black_pixels) if total_black_pixels > 0 else 0.0\n",
    "\n",
    "    # 4) b_4: Proportion of black pixels within a circle of radius 32 (center based on the midline of the image)\n",
    "    # Reuse your previous formula (derive x0 and r with respect to image height)\n",
    "    y0 = H / 2\n",
    "    x_r = 32.0\n",
    "    h_half = H / 2\n",
    "    x0 = (x_r**2 - h_half**2) / (2 * x_r)\n",
    "    r = x_r - x0\n",
    "    yy, xx = np.meshgrid(np.arange(H), np.arange(W), indexing='ij')\n",
    "    circular_mask = ((xx - x0)**2 + (yy - y0)**2) <= (r**2)\n",
    "    black_mask = (gray_img < black_threshold)\n",
    "    black_in_sector = np.sum(circular_mask & black_mask)\n",
    "    b_4 = (black_in_sector / total_black_pixels) if total_black_pixels > 0 else 0.0\n",
    "\n",
    "    return porosity, Lmin, a_4, b_4\n",
    "\n",
    "# ===================== Prediction Loop =====================\n",
    "results = []\n",
    "\n",
    "# If the model does not have built-in preprocess_input, external preprocessing is required (Xceptionâ€™s [-1, 1])\n",
    "if not has_internal_preprocess:\n",
    "    from tensorflow.keras.applications.xception import preprocess_input\n",
    "\n",
    "for image_name in tqdm(image_files, desc=\"Processing images\"):\n",
    "    img_path = os.path.join(test_folder, image_name)\n",
    "    img_bgr = cv2.imread(img_path)\n",
    "    if img_bgr is None:\n",
    "        continue\n",
    "\n",
    "    # ---- Resize to match model requirements ----\n",
    "    img_resized = cv2.resize(img_bgr, (target_w, target_h))\n",
    "\n",
    "    # ---- Compute four auxiliary features using grayscale image \n",
    "    # (based on resized image to match the modelâ€™s input resolution) ----\n",
    "    gray_img = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "    porosity, Lmin, a_4, b_4 = compute_aux_features(gray_img, black_threshold=80)\n",
    "\n",
    "    # ---- Prepare image input ----\n",
    "    # OpenCV loads images as BGR, convert to RGB\n",
    "    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "\n",
    "    if has_internal_preprocess:\n",
    "        # If the model already has Lambda(preprocess_input), skip /255 or external preprocess\n",
    "        img_input = img_rgb  # Keep as float32 in 0~255 range\n",
    "    else:\n",
    "        # If no preprocessing layer in model: apply Xceptionâ€™s preprocess_input ([-1, 1]) at inference\n",
    "        img_input = preprocess_input(img_rgb)\n",
    "\n",
    "    image_input = np.expand_dims(img_input, axis=0)  # [1, H, W, 3]\n",
    "\n",
    "    # ---- Build and normalize the 4 auxiliary features (order must match training) ----\n",
    "    features = np.array([[porosity, Lmin, a_4, b_4]], dtype=np.float32)\n",
    "    features_scaled = ss_input.transform(features)   # shape: [1, 4]\n",
    "\n",
    "    # ---- Feed data depending on the number of model inputs ----\n",
    "    if len(model.inputs) == 2:\n",
    "        pred_scaled = model.predict([image_input, features_scaled], verbose=0)\n",
    "    else:\n",
    "        # If the model has only image input, pass only image_input\n",
    "        pred_scaled = model.predict(image_input, verbose=0)\n",
    "\n",
    "    # ---- Denormalize to original scale ----\n",
    "    # pred_scaled should have shape [1,1]; ss_output must match the one used during training (for single output)\n",
    "    u_pred = ss_output.inverse_transform(pred_scaled.reshape(-1, 1))[0, 0]\n",
    "\n",
    "    results.append([image_name, porosity, Lmin, a_4, b_4, u_pred])\n",
    "\n",
    "# ===================== Save results =====================\n",
    "df_result = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"Image_Name\", \"Porosity\", \"Lmin\", \"a_4\", \"b_4\", \"Predicted_u\"]\n",
    ")\n",
    "df_result.to_excel(output_excel_path, index=False, engine='openpyxl')\n",
    "print(f\"\\nâœ… All predictions completed, results saved to: {output_excel_path}\")\n",
    "\n",
    "# Additional tip: If you want to check the expected input size/channels for the model, you can print:\n",
    "print(f\"Model expected image input size: ({target_h}, {target_w}, 3)\")\n",
    "print(f\"Model contains internal preprocessing layer xcep_pre: {has_internal_preprocess}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf00702-8977-42d5-ae46-7f8635fee805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df5297-66ed-4225-8c0e-021b8f779131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
