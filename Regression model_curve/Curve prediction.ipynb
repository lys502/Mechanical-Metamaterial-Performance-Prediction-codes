{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ddde3-95dc-4e3d-837f-43e02a7380df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import tensorflow\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import keras_tuner as kt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "tensorflow.random.set_seed(20)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a24a3-d739-4f78-be6f-0777db93dd1f",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d1201-1de2-42c1-9858-1740233a0470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the CSV file containing filenames and labels\n",
    "labels = pd.DataFrame(columns=['name'] + ['y' + str(i) for i in range(200)])\n",
    "for file in tqdm(os.listdir('./curves')):\n",
    "    tmp = pd.read_excel(f'./curves/{file}', index_col=0).iloc[1:,].T\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp.columns = ['name'] + ['y' + str(i) for i in range(200)]\n",
    "    labels = pd.concat([labels, tmp], axis=0)\n",
    "labels['name'] = labels['name'] + '.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a01eda0-0f0b-44f6-ac7d-e7f12f7fe4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels2 = pd.read_excel('images_index.xlsx')\n",
    "labels = pd.merge(labels2, labels, how='inner', on='name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b951c5e-d9e3-4768-8864-637765dd7105",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad59670-1506-44bb-80b1-0053b0394cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_csv=labels[labels.subset=='train'][['filename']+['y'+str(i) for i in range(200)]]\n",
    "# test_csv=labels[labels.subset=='test'][['filename']+['y'+str(i) for i in range(200)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390fa32-0a58-4d67-9329-50e6953e8246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "import joblib\n",
    "ss = StandardScaler()\n",
    "labels2 = pd.DataFrame(ss.fit_transform(labels[['y' + str(i) for i in range(200)]]), columns=['y' + str(i) for i in range(200)])\n",
    "for col in ['y' + str(i) for i in range(200)]:\n",
    "    labels[col] = labels2[col]\n",
    "joblib.dump(ss,'ss.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b62320-9556-4a96-868a-ebcaab4c797f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split training and testing sets\n",
    "train_csv, test_csv = train_test_split(labels, test_size=0.3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa5653-bd33-49e2-a365-099d0d40f170",
   "metadata": {},
   "source": [
    "## Display the first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08328da-0d53-4f45-93e0-4cdf23b0bc06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization section\n",
    "plt.figure(figsize=(12, 4), dpi=120)\n",
    "for i in range(1, 11):\n",
    "    y_values = labels.iloc[i-1, 3:203].values  # Change to 4:204 to ensure obtaining 200 y-values\n",
    "    sns.lineplot(x=range(200), y=y_values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aaff4c-ae6c-4c20-a08f-89f710249823",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347bf435-7477-4a9b-864f-ec351e6e8139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom data generator\n",
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, csv_file, directory, batch_size, target_size, label_list, shuffle=True):\n",
    "        self.csv_file = csv_file\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.label_list = label_list\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.csv_file) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.csv_file.iloc[k] for k in indexes]\n",
    "        X, y = self.__data_generation(batch)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.csv_file))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, batch):\n",
    "        X1 = np.empty((self.batch_size, *self.target_size, 3))\n",
    "        X2 = np.empty((self.batch_size, 1))\n",
    "        X3 = np.empty((self.batch_size, 1))\n",
    "        X4 = np.empty((self.batch_size, 1)) \n",
    "        y = np.empty((self.batch_size, len(self.label_list) - 3))\n",
    "\n",
    "        for i, data in enumerate(batch):\n",
    "            img_path = f\"{self.directory}/{data['name']}\"\n",
    "            image = load_img(img_path, target_size=self.target_size)\n",
    "            X1[i,] = img_to_array(image) / 255.0\n",
    "            X2[i,] = data[self.label_list[0]]  # porosity (p)\n",
    "            X3[i,] = data[self.label_list[1]]  # Lmin\n",
    "            X4[i,] = data[self.label_list[2]]  # r\n",
    "            y[i,] = data[self.label_list[3:]]  # curves\n",
    "\n",
    "        return [X1, X2, X3, X4], y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd2c3a5-0f2f-4ee4-aa9a-56441d52febb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the label list, including three auxiliary indicators\n",
    "label_list = ['p', 'lmin', 'r'] + ['y' + str(i) for i in range(200)]\n",
    "batch_size = 16\n",
    "target_size = (256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c01557-5b11-43b5-99c3-ea1740964783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create training and testing data generators\n",
    "train_generator = CustomDataGenerator(train_csv, './images', batch_size, target_size, label_list, shuffle=False)\n",
    "test_generator = CustomDataGenerator(test_csv, './images', batch_size, target_size, label_list, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7b95d-1454-42e3-8521-e65913df9774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a folder to save images\n",
    "os.makedirs('train_images', exist_ok=True)\n",
    "os.makedirs('test_images', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11803f8-8717-49e0-b3f8-f44a5b1e1916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save training set images\n",
    "for i in tqdm(range(len(train_generator))):\n",
    "    batch_data, batch_labels = train_generator[i]\n",
    "    for j, img_array in enumerate(batch_data[0]):\n",
    "        img_name = train_csv.iloc[i * batch_size + j]['name']\n",
    "        img = array_to_img(img_array)\n",
    "        img.save(os.path.join('train_images', img_name))\n",
    "\n",
    "for i in tqdm(range(len(test_generator))):\n",
    "    batch_data, batch_labels = test_generator[i]\n",
    "    for j, img_array in enumerate(batch_data[0]):\n",
    "        img_name = test_csv.iloc[i * batch_size + j]['name']\n",
    "        img = array_to_img(img_array)\n",
    "        img.save(os.path.join('test_images', img_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a410867-2a18-48b6-845c-38a0bf8b98c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if GPU is enabled\n",
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()  \n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')  \n",
    "for device in physical_devices:\n",
    "     print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd58e99-4167-4063-8a33-7be2541bceda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract label data from the training and testing sets\n",
    "y_train = []\n",
    "for i in tqdm(range(len(train_generator))): \n",
    "    batch_data, batch_labels = train_generator[i]\n",
    "    y_train.append(batch_labels)\n",
    "y_train = np.concatenate(y_train)\n",
    "\n",
    "y_test = []\n",
    "for i in tqdm(range(len(test_generator))):\n",
    "    batch_data, batch_labels = test_generator[i]\n",
    "    y_test.append(batch_labels)\n",
    "y_test = np.concatenate(y_test)\n",
    "y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d8439b-a45f-4347-9485-062bca21ce1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train.shape,y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6aa51-0594-4f3f-9793-e001f31bb09c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(10,3),dpi=120)\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['mse'])+1),model_history.history['mse'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_mse'])+1),model_history.history['val_mse'])\n",
    "    axs[0].set_title('Model mse')\n",
    "    axs[0].set_ylabel('mse')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    # axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])/10)\n",
    "    # axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    # axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    fig.savefig('Model iteration chart.jpg',dpi=600, bbox_inches = 'tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d0a68-a02a-448e-97cd-00778a501e0e",
   "metadata": {},
   "source": [
    "## Automatic hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7f66f-019a-489e-9ae1-6ef7f84b4c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modify the model-building function to support two auxiliary inputs\n",
    "def build_model(hp):\n",
    "    input_image = Input(shape=(256, 256, 3))\n",
    "    input_features1 = Input(shape=(1,))  # p\n",
    "    input_features2 = Input(shape=(1,))  # Lmin\n",
    "    input_features3 = Input(shape=(1,))  # r\n",
    "\n",
    "    base_model = tf.keras.applications.Xception(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "    x = base_model(input_image)\n",
    "    x = Flatten()(x)\n",
    "    x = Concatenate()([x, input_features1, input_features2, input_features3])\n",
    "    \n",
    "    for i in range(hp.Int('num_layers', 2, 2)):\n",
    "        x = Dense(units=hp.Int(f'units_{i}', min_value=128, max_value=512, step=32), \n",
    "                  activation=hp.Choice(f'activation_{i}', values=['relu', 'tanh']))(x)\n",
    "        x = Dropout(hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1))(x)\n",
    "\n",
    "    output = Dense(200)(x)\n",
    "    model = Model(inputs=[input_image, input_features1, input_features2, input_features3], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-3, sampling='LOG')),\n",
    "        loss='mse',\n",
    "        metrics=['mse']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=100,  \n",
    "    factor=3,\n",
    "    directory='my_dir',\n",
    "    project_name='helloworld'\n",
    ")\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(train_generator, epochs=15, validation_data=test_generator, callbacks=[stop_early])\n",
    "# Obtain the optimal hyperparameter combination\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] \n",
    "\n",
    "# Print the optimal hyperparameters\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of layers is {best_hps.get('num_layers')} \n",
    "with the following configuration:\n",
    "\"\"\")\n",
    "for i in range(best_hps.get('num_layers')):\n",
    "    print(f\"Layer {i + 1}: {best_hps.get(f'units_{i}')} units, {best_hps.get(f'activation_{i}')} activation, {best_hps.get(f'dropout_{i}')} dropout\")\n",
    "\n",
    "print(f\"Optimal learning rate: {best_hps.get('learning_rate')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d591431-c04a-46d0-8236-9cbb9a6daf0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def build_model(hp):\n",
    "#     input_image = Input(shape=(256, 256, 3))\n",
    "#     input_features = Input(shape=(1,))\n",
    "#     base_model = tf.keras.applications.Xception(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "#     x = base_model(input_image)\n",
    "#     x = Flatten()(x)\n",
    "#     x = Concatenate()([x, input_features])\n",
    "    \n",
    "#     for i in range(hp.Int('num_layers', 2, 3)):\n",
    "#         x = Dense(units=hp.Int(f'units_{i}', min_value=128, max_value=512, step=32), \n",
    "#                   activation=hp.Choice(f'activation_{i}', values=['relu', 'tanh']))(x)\n",
    "\n",
    "#     output = Dense(200)(x)\n",
    "#     model = Model(inputs=[input_image, input_features], outputs=output)\n",
    "#     model.compile(\n",
    "#         optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-3, sampling='LOG')),\n",
    "#         loss='mse',\n",
    "#         metrics=['mse']\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# tuner = kt.Hyperband(\n",
    "#     build_model,\n",
    "#     objective='val_loss',\n",
    "#     max_epochs=3,  \n",
    "#     factor=3,\n",
    "#     directory='my_dir',\n",
    "#     project_name='helloworld'\n",
    "# )\n",
    "\n",
    "# stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "# tuner.search(train_generator, epochs=15, validation_data=test_generator, callbacks=[stop_early])  \n",
    "\n",
    "# best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# print(f\"\"\"\n",
    "# The hyperparameter search is complete. The optimal number of layers is {best_hps.get('num_layers')} \n",
    "# with the following configuration:\n",
    "# \"\"\")\n",
    "# for i in range(best_hps.get('num_layers')):\n",
    "#     print(f\"Layer {i + 1}: {best_hps.get(f'units_{i}')} units, {best_hps.get(f'activation_{i}')} activation\")\n",
    "\n",
    "# print(f\"Optimal learning rate: {best_hps.get('learning_rate')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a6cf0-069d-46c0-b8b0-fd9c74119943",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build and train the best model\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=100,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(test_generator),\n",
    "    callbacks=[tf.keras.callbacks.ModelCheckpoint(\"./best_model.h5\", save_best_only=True, monitor='val_loss')]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0815fda-5674-47c4-b432-6404eb9494ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f258-8058-453d-a93b-019a21d47b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export loss values to a CSV file:\n",
    "loss_values = history.history['loss'] \n",
    "val_loss_values = history.history['val_loss'] \n",
    "\n",
    "num_epoch = len(loss_values)\n",
    "\n",
    "df_loss = pd.DataFrame({\n",
    "    'epoch': np.arange(1, num_epoch + 1),\n",
    "    'loss': loss_values,\n",
    "    'val_loss': val_loss_values\n",
    "})\n",
    "\n",
    "df_loss.to_csv('loss_curve.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a87c5-775e-4c1b-a612-4bb6d069ece7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "model.save('my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c70e6-3d70-4c3f-be60-d76ade5aecd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load model\n",
    "model = load_model('my_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf457c54-466b-4ffb-91a2-6827b20c9c74",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e4df95-4fc2-465e-ace6-f007f7033bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model and output the results\n",
    "y_test_pred = model.predict(test_generator)\n",
    "y_train_pred = model.predict(train_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954da29-3e06-48d3-9bf6-51cb3322d7fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate R² value and area ratio\n",
    "from sklearn import metrics\n",
    "r2_list = []\n",
    "area_list = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    r2_list.append(round(metrics.r2_score(ss.inverse_transform(y_test)[i, :], ss.inverse_transform(y_test_pred)[i, :]), 4))\n",
    "    area_list.append(np.trapz(ss.inverse_transform(y_test_pred)[i, :], range(200)) / np.trapz(ss.inverse_transform(y_test)[i, :]))\n",
    "\n",
    "test_r2 = pd.DataFrame({'test_r2': r2_list})['test_r2']\n",
    "test_r2_mean = test_r2.mean()\n",
    "test_r2\n",
    "test_r2_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64231931-f231-46c0-8ee8-a53b002c377d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export the R² values to an Excel file\n",
    "test_r2.to_excel('image_r2_values.xlsx', index=False)\n",
    "\n",
    "print(\"R² to image_r2_values.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ee716-2c59-4ebe-8e93-32cd0affc53a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_area = pd.DataFrame({'area_list': area_list})['area_list']\n",
    "test_area_mean = test_area.mean()\n",
    "test_area\n",
    "test_area_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38dee44-b403-4cd2-bdb0-5cc87ad690c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export the true and predicted values, including image names\n",
    "y_test_df = pd.DataFrame(ss.inverse_transform(y_test))\n",
    "y_test_pred_df = pd.DataFrame(ss.inverse_transform(y_test_pred))\n",
    "\n",
    "y_test_df.insert(0, 'name', test_csv['name'].reset_index(drop=True))\n",
    "y_test_pred_df.insert(0, 'name', test_csv['name'].reset_index(drop=True))\n",
    "\n",
    "y_test_df.to_excel('True-test.xlsx', index=False)\n",
    "y_test_pred_df.to_excel('Pred-test.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3853e-e490-48f0-b924-02fef252a802",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#True vs. predicted comparison plot\n",
    "import seaborn as sns\n",
    "for i in range(10):\n",
    "    plt.figure(figsize=(8,3), dpi=120)\n",
    "    sns.lineplot(x=range(200),y=ss.inverse_transform(y_test)[i,:],label='True')\n",
    "    sns.lineplot(x=range(200),y=ss.inverse_transform(y_test_pred)[i,:],label='Preict')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79e4add-cccd-4e00-b69a-3db4f379d23e",
   "metadata": {},
   "source": [
    "## Interval classification section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a45ad9-9db9-4e41-a02d-100976c37026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom data generator\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "encoder = OneHotEncoder(sparse_output=False) \n",
    "encoder.fit(labels[['r']])\n",
    "\n",
    "\n",
    "class CustomDataGenerator2(Sequence):\n",
    "    def __init__(self, csv_file, directory, batch_size, target_size, label_list, shuffle=True):\n",
    "        self.csv_file = csv_file\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.label_list = label_list\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.csv_file) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.csv_file.iloc[k] for k in indexes]\n",
    "        X, y = self.__data_generation(batch)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.csv_file))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, batch):\n",
    "        X1 = np.empty((self.batch_size, *self.target_size, 3))\n",
    "        X2 = np.empty((self.batch_size, 1))  # p\n",
    "        X3 = np.empty((self.batch_size, 1))  # Lmin\n",
    "        y = np.empty((self.batch_size, len(encoder.categories_[0])))  \n",
    "\n",
    "        for i, data in enumerate(batch):\n",
    "            img_path = f\"{self.directory}/{data['name']}\"\n",
    "            image = load_img(img_path, target_size=self.target_size)\n",
    "            X1[i,] = img_to_array(image) / 255.0\n",
    "            X2[i,] = data[self.label_list[0]]  # p\n",
    "            X3[i,] = data[self.label_list[1]]  # Lmin\n",
    "            y[i,] = encoder.transform([[data[self.label_list[2]]]])[0]\n",
    "\n",
    "        return [X1, X2, X3], y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29456335-8302-45da-8bec-603b993eba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbdc660-10d0-4748-a92b-80143b3109fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_list2 = ['p', 'Lmin', 'r']\n",
    "batch_size = 16\n",
    "target_size = (256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad035e-4e7e-4e06-b5a3-efa7397eaf7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_generator2 = CustomDataGenerator2(train_csv, './images', batch_size, target_size, label_list2, shuffle=False)\n",
    "test_generator2 = CustomDataGenerator2(test_csv, './images', batch_size, target_size, label_list2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d6673-9918-46b7-a7be-c670ea2a950f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for i in tqdm(range(len(train_generator2))):\n",
    "    batch_data, batch_labels = train_generator2[i]\n",
    "    y_train.append(batch_labels)\n",
    "y_train = np.concatenate(y_train)\n",
    "\n",
    "y_test = []\n",
    "for i in tqdm(range(len(test_generator2))):\n",
    "    batch_data, batch_labels = test_generator2[i]\n",
    "    y_test.append(batch_labels)\n",
    "y_test = np.concatenate(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b38814-7575-4033-8b6c-e0fd5d78d248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train.shape,y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeba59f-cb1f-4455-a11f-be05e3fb494b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e32553-02ef-4dd6-9db6-7c30c587fe0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model2(hp):\n",
    "    input_image = Input(shape=(256, 256, 3))\n",
    "    input_features1 = Input(shape=(1,))  \n",
    "    input_features2 = Input(shape=(1,))  \n",
    "    \n",
    "    base_model = tf.keras.applications.Xception(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "    x = base_model(input_image)\n",
    "    x = Flatten()(x)\n",
    "    x = Concatenate()([x, input_features1, input_features2])  \n",
    "    \n",
    "    for i in range(hp.Int('num_layers', 2, 2)):\n",
    "        x = Dense(units=hp.Int(f'units_{i}', min_value=128, max_value=512, step=32), \n",
    "                  activation=hp.Choice(f'activation_{i}', values=['relu', 'tanh']))(x)\n",
    "        x = Dropout(hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1))(x)\n",
    "\n",
    "    output = Dense(len(encoder.categories_[0]), activation='softmax')(x)\n",
    "    model = Model(inputs=[input_image, input_features1, input_features2], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-4, sampling='LOG')),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_model2,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=100,\n",
    "    factor=3,\n",
    "    directory='my_dir2',\n",
    "    project_name='helloworld2'\n",
    ")\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "tuner.search(train_generator2, epochs=15, validation_data=test_generator2, callbacks=[stop_early])\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of layers is {best_hps.get('num_layers')} \n",
    "with the following configuration:\n",
    "\"\"\")\n",
    "for i in range(best_hps.get('num_layers')):\n",
    "    print(f\"Layer {i + 1}: {best_hps.get(f'units_{i}')} units, {best_hps.get(f'activation_{i}')} activation, {best_hps.get(f'dropout_{i}')} dropout\")\n",
    "\n",
    "print(f\"Optimal learning rate: {best_hps.get('learning_rate')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e63f9c-42cc-48ef-a7ea-5456c084b472",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = tuner.hypermodel.build(best_hps)\n",
    "history2 = model2.fit(\n",
    "    train_generator2,\n",
    "    steps_per_epoch=len(train_generator2),\n",
    "    epochs=100,\n",
    "    validation_data=test_generator2,\n",
    "    validation_steps=len(test_generator2),\n",
    "    callbacks=[tf.keras.callbacks.ModelCheckpoint(\"./best_model2.h5\", save_best_only=True, monitor='val_accuracy')]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e6f86-695e-4e99-98dd-cddbf14f209b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model_history2(model_history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 3), dpi=120)\n",
    "    axs[0].plot(range(1, len(model_history.history['accuracy']) + 1), model_history.history['accuracy'])\n",
    "    axs[0].plot(range(1, len(model_history.history['val_accuracy']) + 1), model_history.history['val_accuracy'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    \n",
    "    axs[1].plot(range(1, len(model_history.history['loss']) + 1), model_history.history['loss'])\n",
    "    axs[1].plot(range(1, len(model_history.history['val_loss']) + 1), model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()\n",
    "plot_model_history2(history2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa338ca-8131-44f3-a5d0-6169b83096c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_values = history2.history['accuracy'] \n",
    "val_accuracy_values = history2.history['val_accuracy']  \n",
    "loss_values = history2.history['loss']  \n",
    "val_loss_values = history2.history['val_loss'] \n",
    "\n",
    "num_epoch = len(accuracy_values)\n",
    "\n",
    "df_metrics = pd.DataFrame({\n",
    "    'epoch': np.arange(1, num_epoch + 1),\n",
    "    'accuracy': accuracy_values,\n",
    "    'val_accuracy': val_accuracy_values,\n",
    "    'loss': loss_values,\n",
    "    'val_loss': val_loss_values\n",
    "})\n",
    "\n",
    "df_metrics.to_csv('classification_training_curve.csv', index=False)\n",
    "\n",
    "print(\" classification_training_curve.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c81448-309f-4ffc-9eec-2df7a86b06de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2.save('my_model2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f66577-9b45-4e9c-a990-e897c0f63dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model2 = tf.keras.models.load_model('my_model2.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5c77b-6c44-430a-9206-7d427faf61cd",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727e404-b0fa-4455-b477-bbed15fa1bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score ,roc_curve, auc,confusion_matrix ,accuracy_score,roc_auc_score,auc,brier_score_loss\n",
    "from sklearn import metrics\n",
    "def try_different_method(y_train,y_pred_train1,y_test,y_pred_test1):\n",
    "    print('Train:')\n",
    "    precision = precision_score(y_train,y_pred_train1,average='macro')\n",
    "    recall = recall_score(y_train,y_pred_train1,average='macro')\n",
    "    f1score = f1_score(y_train, y_pred_train1,average='macro')\n",
    "    accuracy=accuracy_score(y_train, y_pred_train1)\n",
    "    print(\"ACC: \", '%.4f'%float(accuracy),\"F1：\", '%.4f'%float(f1score),\"Precision:\", '%.4f'%float(precision),\\\n",
    "    \"Recall:   \",'%.4f'%float(recall))\n",
    "    train_metrics=[accuracy,precision,recall,f1score]\n",
    "    print('Model Train Report: \\n',metrics.classification_report(y_train,y_pred_train1,digits=4))\n",
    "    print('*'*50)\n",
    "    print('Test:')\n",
    "\n",
    "    precision = precision_score(y_test,y_pred_test1,average='macro')\n",
    "    recall = recall_score(y_test,y_pred_test1,average='macro')\n",
    "    f1score = f1_score(y_test, y_pred_test1,average='macro')\n",
    "    accuracy=accuracy_score(y_test, y_pred_test1)\n",
    "    print(\"ACC: \", '%.4f'%float(accuracy),\"F1：\", '%.4f'%float(f1score),\"Precision:\", '%.4f'%float(precision),\\\n",
    "    \"Recall:   \",'%.4f'%float(recall))\n",
    "    print('Model Test Report: \\n',metrics.classification_report(y_test,y_pred_test1,digits=4))\n",
    "    test_metrics=[accuracy,precision,recall,f1score]\n",
    "    return train_metrics,test_metrics\n",
    "import matplotlib.pyplot as plt \n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "    # plt.figure(figsize=(12,6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\",color=\"#00FFFF\" if cm[i, j] > thresh else \"red\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de31b84-69ab-43e0-b512-6764f34d5fff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the classification model and output the results\n",
    "y_test_pred = model2.predict(test_generator2)\n",
    "y_train_pred = model2.predict(train_generator2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8fea9-c376-4553-b455-2492a59d54ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels['r'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2556d65c-b44e-4d76-92e7-5e31fcaffc5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_metrics,test_metrics=try_different_method(np.argmax(y_train,axis=1),np.argmax(y_train_pred,axis=1),\n",
    "                                                np.argmax(y_test,axis=1),np.argmax(y_test_pred,axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c93c367-2332-4e69-90d8-88a8e8e49c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test_df = pd.DataFrame(np.argmax(y_test,axis=1)+3)\n",
    "y_test_pred_df = pd.DataFrame(np.argmax(y_test_pred,axis=1)+3)\n",
    "\n",
    "y_test_df.insert(0, 'name', test_csv['name'].reset_index(drop=True))\n",
    "y_test_pred_df.insert(0, 'name', test_csv['name'].reset_index(drop=True))\n",
    "\n",
    "y_test_df.to_excel('True-test2.xlsx', index=False)\n",
    "y_test_pred_df.to_excel('Pred-test2.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62a33c-1e0d-4b0b-888c-2192d80bc70c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6), dpi=120)\n",
    "plt.subplot(2, 2, 1)\n",
    "#train\n",
    "cnf_matrix=metrics.confusion_matrix(np.argmax(y_train,axis=1),np.argmax(y_train_pred,axis=1))\n",
    "plot_confusion_matrix(cnf_matrix,[3,4,5,6,7],title='Train',cmap=plt.cm.Blues)\n",
    "#test\n",
    "plt.subplot(2, 2, 2)\n",
    "cnf_matrix=metrics.confusion_matrix(np.argmax(y_test,axis=1),np.argmax(y_test_pred,axis=1))\n",
    "plot_confusion_matrix(cnf_matrix,[3,4,5,6,7],title='Test',cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.jpg',dpi=300,bbox_inches = 'tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b8ed4-1e7b-4039-94e1-ee03a7ad9e89",
   "metadata": {},
   "source": [
    "## Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882b834-814f-4c72-a91d-94da676dc17f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model1 = load_model('my_model.h5')  # Load curve prediction model\n",
    "model2 = load_model('my_model2.h5')  # Load r prediction model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3749c6-42c5-4a0e-8932-a446203c6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model1 = load_model('my_model.h5')\n",
    "model2 = load_model('my_model2.h5')\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = joblib.load('ss.pkl')\n",
    "os.makedirs('./predicted_curves', exist_ok=True) \n",
    "\n",
    "predictions = []\n",
    "image_names = []\n",
    "\n",
    "# Traverse images in the new folder\n",
    "new_image_folder = './val_images/'  # New image folder path\n",
    "for img_name in os.listdir(new_image_folder):\n",
    "    # Read image\n",
    "    img_path = os.path.join(new_image_folder, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "\n",
    "    # Calculate porosity\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  \n",
    "    white_pixels = np.sum(gray_img > 80)  \n",
    "    total_pixels = gray_img.size  # \n",
    "    p_value = np.array([[white_pixels / total_pixels]]) \n",
    "    \n",
    "    # Calculate Lmin\n",
    "    height, width = gray_img.shape\n",
    "    min_black_count = float('inf')\n",
    "    for y in range(height):\n",
    "        row = gray_img[y, :]\n",
    "        is_black = (row < 127)\n",
    "        black_count = is_black.sum()\n",
    "        if black_count < min_black_count:\n",
    "            min_black_count = black_count\n",
    "    Lmin_value = np.array([[min_black_count]]) \n",
    "\n",
    "    img = np.reshape(img, [1, 256, 256, 3]) / 255.0\n",
    "\n",
    "    # Call model2 to predict the interval, passing all three inputs\n",
    "    r_value = np.argmax(model2.predict([img, p_value, Lmin_value]), axis=1)[0] + 3\n",
    "\n",
    "    # Predict the curve by passing the image, p, Lmin, and r\n",
    "    prediction = model1.predict([img, p_value, Lmin_value, np.array([[r_value]])])\n",
    "\n",
    "    \n",
    "    inverse_prediction = ss.inverse_transform(prediction)[0]\n",
    "\n",
    " \n",
    "    predictions.append(inverse_prediction)\n",
    "    image_names.append(img_name)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(range(200), inverse_prediction)  \n",
    "    plt.title(f'Predicted Curve for {img_name}')\n",
    "    plt.xlabel('Data Points')\n",
    "    plt.ylabel('Predicted Value')\n",
    "    plt.savefig(f'./predicted_curves/{img_name}_curve.png')  \n",
    "    plt.close()\n",
    "\n",
    "# 导出所有预测数据到Excel\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.insert(0, 'Image Name', image_names)  \n",
    "predictions_df.to_excel('predicted_curves_data.xlsx', index=False)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
