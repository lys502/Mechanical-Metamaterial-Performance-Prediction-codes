{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8e5b9-b2db-41a8-bda4-d01ec3008d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import shutil, random, warnings, joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.random.set_seed(20)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40419687-4e01-4466-95ee-343a4dc4a49e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebcaeb-cfab-4ce6-b891-60c479b24d29",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a77aa4-09de-4fde-ab2e-ba9bc47459dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================== data ======================\n",
    "labels = pd.read_excel('data.xlsx')\n",
    "\n",
    "\n",
    "label_list2 = ['u', 'PCF', 'EA', 'SEA', 'k']\n",
    "\n",
    "\n",
    "ss = MinMaxScaler()\n",
    "labels[label_list2] = ss.fit_transform(labels[label_list2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b258eb-a64d-4973-a8d2-11f4922a232f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c966d8a-cddf-49a9-98b1-86f1361b8b07",
   "metadata": {},
   "source": [
    "## Create training and testing data – run once only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32e649-1908-4e5f-b1e9-380222ba0765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# image_folder = \"./images\"\n",
    "# train_folder = './train'\n",
    "# test_folder = './test'\n",
    "\n",
    "# if os.path.exists(train_folder): shutil.rmtree(train_folder)\n",
    "# if os.path.exists(test_folder): shutil.rmtree(test_folder)\n",
    "# os.makedirs(train_folder, exist_ok=True)\n",
    "# os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "# image_files = [i for i in os.listdir(image_folder) if i.endswith('jpg')]\n",
    "# random.shuffle(image_files)\n",
    "# split_idx = int(0.7 * len(image_files))\n",
    "# train_images = image_files[:split_idx]\n",
    "# test_images = image_files[split_idx:]\n",
    "\n",
    "# for img in train_images:\n",
    "#     shutil.copy(os.path.join(image_folder, img), os.path.join(train_folder, img))\n",
    "# for img in test_images:\n",
    "#     shutil.copy(os.path.join(image_folder, img), os.path.join(test_folder, img))\n",
    "\n",
    "# train_csv = labels[labels.name.isin(train_images)]\n",
    "# test_csv = labels[labels.name.isin(test_images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54031b6e-8afd-4d24-a843-c0dd190e546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistent with multimodal Xception\n",
    "train_folder = './train'\n",
    "test_folder = './test'\n",
    "\n",
    "\n",
    "train_images = [f for f in os.listdir(train_folder) if f.endswith('.jpg')]\n",
    "test_images = [f for f in os.listdir(test_folder) if f.endswith('.jpg')]\n",
    "\n",
    "\n",
    "train_csv = labels[labels.name.isin(train_images)]\n",
    "test_csv = labels[labels.name.isin(test_images)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d68a8-d237-4045-b5ca-05555821b2d7",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f7c97-6dcd-41c2-9f9a-386f916c34ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================== image only ======================\n",
    "class ImageDataGeneratorOnlyImage(Sequence):\n",
    "    def __init__(self, csv_file, directory, batch_size, target_size, label_list, shuffle=True, augment=False):\n",
    "        self.csv_file = csv_file.reset_index(drop=True)\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.label_list = label_list\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        self.image_data_generator = ImageDataGenerator(\n",
    "            rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "            shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest'\n",
    "        ) if self.augment else ImageDataGenerator()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.csv_file) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch = self.csv_file.iloc[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X = np.empty((len(batch), *self.target_size, 3))\n",
    "        y = np.empty((len(batch), len(self.label_list)))\n",
    "\n",
    "        for i, data in enumerate(batch.itertuples()):\n",
    "            img_path = os.path.join(self.directory, data.name)\n",
    "            image = load_img(img_path, target_size=self.target_size)\n",
    "            # image = img_to_array(image) / 255.0\n",
    "            image = img_to_array(image)\n",
    "            \n",
    "            if self.augment: image = self.image_data_generator.random_transform(image)\n",
    "            X[i] = image\n",
    "            y[i] = np.array([getattr(data, col) for col in self.label_list])\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.csv_file = self.csv_file.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd8281f-295c-4b67-b2bf-6399afcf5a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "target_size = (256, 256)\n",
    "train_generator = ImageDataGeneratorOnlyImage(train_csv, train_folder, batch_size, target_size, label_list2, shuffle=True, augment=True)\n",
    "test_generator = ImageDataGeneratorOnlyImage(test_csv, test_folder, batch_size, target_size, label_list2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4849d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()  \n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU') \n",
    "for device in physical_devices:  \n",
    "     print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94889cc-616d-43da-96d1-96d36e820ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model_history(model_history,model_name):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(12,4),dpi=120)\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['mse'])+1),model_history.history['mse'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_mse'])+1),model_history.history['val_mse'])\n",
    "    axs[0].set_title('Model mse')\n",
    "    axs[0].set_ylabel('mse')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    # axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])/10)\n",
    "    # axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    # axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    fig.savefig(model_name+'_loss.jpg',dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c6214-052e-4a89-bfb6-297ed7faa6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_model(hp):\n",
    "    from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
    "    input_image = Input(shape=(299, 299, 3))\n",
    "\n",
    "    # Official preprocessing ([-1, 1]); must match generator without /255.0\n",
    "    x_in = tf.keras.layers.Lambda(preprocess_input, name='xcep_pre')(input_image)\n",
    "\n",
    "    # Xception backbone (no top layer), GAP is more stable (can also switch to Flatten for equivalent comparison)\n",
    "    base_model = Xception(weights='imagenet', include_top=False, input_shape=(299, 299, 3), pooling='avg')\n",
    "\n",
    "    # Freeze only BN layers to avoid small batch statistics corruption; forward pass fixed to inference mode\n",
    "    for l in base_model.layers:\n",
    "        if isinstance(l, tf.keras.layers.BatchNormalization):\n",
    "            l.trainable = False\n",
    "    x = base_model(x_in, training=False)   # BN uses inference statistics\n",
    "\n",
    "    # Three fully connected layers: relu → tanh → relu (units searched as hyperparameters)\n",
    "    x = Dense(hp.Int('units_0', 96, 512, step=32), activation='relu')(x)\n",
    "    x = Dense(hp.Int('units_1', 96, 512, step=32), activation='tanh')(x)\n",
    "    x = Dense(hp.Int('units_2', 96, 512, step=32), activation='relu')(x)\n",
    "\n",
    "    output = Dense(len(label_list2))(x)\n",
    "    model = Model(inputs=input_image, outputs=output)\n",
    "\n",
    "    # Slightly loosen the lower bound of learning rate for more stability\n",
    "    lr = hp.Float('lr', min_value=3e-5, max_value=1e-3, sampling='LOG')\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# ====================== Hyperparameter Search ======================\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=8,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='xception_tuning_no_porosity'\n",
    ")\n",
    "\n",
    "tuner.search(train_generator, epochs=10, validation_data=test_generator)\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "print(f\"Learning rate: {best_hps.get('lr')}\")\n",
    "print(f\"Layer 1 units: {best_hps.get('units_0')} Activation: relu\")\n",
    "print(f\"Layer 2 units: {best_hps.get('units_1')} Activation: tanh\")\n",
    "print(f\"Layer 3 units: {best_hps.get('units_2')} Activation: relu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182b15c-958b-4110-92f1-cf1274f68fac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint(\"./model.ckpt\", monitor='val_loss', verbose=1,\n",
    "#                                                 save_best_only=True, mode='min', save_weights_only=True)\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"best_model.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min', save_weights_only=False\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775986d-c128-443f-9d76-59137ddea690",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_info = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=100,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(test_generator),\n",
    "    callbacks=[checkpoint, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f98be1-8b57-442a-9741-c87f72e4bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_model_history(model_history, model_name):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4), dpi=120)\n",
    "    axs[0].plot(model_history.history['mse'], label='Train MSE')\n",
    "    axs[0].plot(model_history.history['val_mse'], label='Val MSE')\n",
    "    axs[0].set_title('Model MSE')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(model_history.history['loss'], label='Train Loss')\n",
    "    axs[1].plot(model_history.history['val_loss'], label='Val Loss')\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].legend()\n",
    "\n",
    "    fig.savefig(model_name + '_loss.jpg', dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "plot_model_history(model_info, 'Xception_no_porosity')\n",
    "\n",
    "hist = model_info.history\n",
    "loss_df = pd.DataFrame({\n",
    "    'epoch': np.arange(1, len(hist['loss'])+1),\n",
    "    'train_loss': hist['loss'],\n",
    "    'val_loss': hist['val_loss'],\n",
    "    'train_mse': hist.get('mse', hist.get('mean_squared_error')),\n",
    "    'val_mse': hist.get('val_mse', hist.get('val_mean_squared_error'))\n",
    "})\n",
    "loss_df.to_excel('loss_curve_data.xlsx', index=False)\n",
    "\n",
    "print(\"loss data to loss_curve_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b914e-cd22-4a1d-bae6-74a488c6314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.load_model(\"best_model.h5\")\n",
    "y_test_true = np.vstack([y for _, y in test_generator])\n",
    "y_test_pred = model.predict(test_generator)\n",
    "\n",
    "\n",
    "y_test_true = ss.inverse_transform(y_test_true)\n",
    "y_test_pred = ss.inverse_transform(y_test_pred)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "for i, label in enumerate(label_list2):\n",
    "    mse = mean_squared_error(y_test_true[:, i], y_test_pred[:, i])\n",
    "    mae = mean_absolute_error(y_test_true[:, i], y_test_pred[:, i])\n",
    "    r2 = r2_score(y_test_true[:, i], y_test_pred[:, i])\n",
    "    print(f\"{label} -> MSE:{mse:.4f}  MAE:{mae:.4f}  R2:{r2:.4f}\")\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "color_list = ['red', 'blue', 'purple', 'orange', 'green']\n",
    "plt.figure(figsize=(12, 10), dpi=120)\n",
    "for i, label in enumerate(label_list2):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.scatter(y_test_pred[:, i], y_test_true[:, i], color=color_list[i])\n",
    "    plt.title(label)\n",
    "    plt.xlabel('Predict')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "plt.savefig('no_porosity.jpg', dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "with pd.ExcelWriter('true_vs_predicted_comparison_no_porosity.xlsx') as writer:\n",
    "    for i, label in enumerate(label_list2):\n",
    "        df_comparison = pd.DataFrame({\n",
    "            'True_Value': y_test_true[:, i].ravel(),\n",
    "            'Predicted_Value': y_test_pred[:, i].ravel()\n",
    "        })\n",
    "        df_comparison.to_excel(writer, sheet_name=label, index=False)\n",
    "\n",
    "\n",
    "joblib.dump(ss, 'scaler_no_porosity.pkl')\n",
    "print(\"Model and scaler saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a276af-c6ff-4a8e-832c-ba4176e97f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b80470-035c-4be2-a2ff-fdca5f1d2e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de546d-f2d1-4634-878f-84418c3ff52d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
